{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58728b43",
   "metadata": {},
   "source": [
    "# CS4243 Project\n",
    "\n",
    "1. Clement Ng\n",
    "2. Kimberley Tay\n",
    "3. Wayne Tan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabb19b4",
   "metadata": {},
   "source": [
    "## Scraping dataset\n",
    "\n",
    "1. Images are scraped from google images, duckduckgo images, and unsplash.\n",
    "2. They are then manually labelled by us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5a950afd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from scraper import main\n",
    "\n",
    "#main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a586a9",
   "metadata": {},
   "source": [
    "## Image Preprocessing\n",
    "\n",
    "In this step, we will be taking a look at\n",
    "1. The dataset distribution\n",
    "2. Normalising of dataset\n",
    "3. Saving the dataset in a .pt extension for easily reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86e9511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import datasets, transforms\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time, os, torch, shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468cc33e",
   "metadata": {},
   "source": [
    "### Checking sizes of images\n",
    "\n",
    "A few things to note:\n",
    "1. base_folder defines the folder in which all the images are in.\n",
    "2. It is assumed that the images are in their respective folders based on their labels, with the folder name being the label, and all of them are in base_folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49470f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = './dataset-raw'\n",
    "classes = os.listdir(base_folder)\n",
    "classes = [int(i) for i in classes]\n",
    "classes.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b583cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_stats(classes):\n",
    "    stats = []\n",
    "    paths = []\n",
    "    for class_ in classes:\n",
    "        folder_path = f'{base_folder}/{class_}'\n",
    "        files = os.listdir(folder_path)\n",
    "        files.sort()\n",
    "        for file in files:\n",
    "            file_path = f'{folder_path}/{file}'\n",
    "            im = Image.open(file_path)\n",
    "            size = im.size\n",
    "            stats.append(size + (min(class_, 11), ))\n",
    "            paths.append(file_path)\n",
    "    return np.array(stats), np.array(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "465db8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats, paths = dataset_stats(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2ce7a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(x=stats[:, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f475ce1",
   "metadata": {},
   "source": [
    "We observe that the amount of dataset for label 1 is much higher than the rest, so the measure we have chosen to adopt here is to deploy undersampling. Furthermore, for the labels >6, the quantity of dataset available is alot less than those <=6, and based on our decision that 6 is a good number to cut off. The label choice we have decided on will be [0, 1, 2, 3, 4, 5, 6, 7-10,>10].\n",
    "\n",
    "We have determined that it doesnt make a difference whether there 11 or 12 people in an image, and 6 makes for a size suitable for a small group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7c85421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratio(stats):\n",
    "    result = stats[:, 0:2].max(axis=1) / stats[:, 0:2].min(axis=1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afbe0c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ratios = get_ratio(stats)\n",
    "sns.histplot(x=ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16f5f8f",
   "metadata": {},
   "source": [
    "Most of the images lie within a boundary of 1 to 2 in aspect ratios, and since we decided to conduct undersampling for majority classes, we will prioritise keeping the images whose aspect ratio lie in this range. In other words, the images whose aspect ratios lie outside this range will be removed with priority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f315c138",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(stats)\n",
    "train_proportion = 0.8\n",
    "train_size = int(train_proportion*size)\n",
    "np.random.seed(5)\n",
    "shuffled_indices = np.random.permutation(np.arange(size))\n",
    "train_indices = shuffled_indices[:train_size]\n",
    "test_indices = shuffled_indices[train_size:]\n",
    "train_set_stats = stats[train_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91507f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[339. 249.   2.]\n"
     ]
    }
   ],
   "source": [
    "print(np.median(train_set_stats, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8863db7",
   "metadata": {},
   "source": [
    "Since the smaller dimension of the median is 249 and we note that 224 x 224 is a common dimension for images resizing for CNN, we have chosen to adopt 224 x 224 as our dimension for images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8aed17",
   "metadata": {},
   "source": [
    "### Filtering Images\n",
    "\n",
    "There are images which are very small compared to the size we have have chosen to keep, which is 224x224. So we will filter the images which are less than 1/4 of the desired size for all.\n",
    "\n",
    "Furthermore, since there is a majority of dataset for label 0, 1 and 2, we have decided to undersample it, in the following order:\n",
    "1. Selectively filter based on aspect ratio (so that the amount of black box in the dataset would not be too high.\n",
    "2. Keep filtering until we have approximately equal to the number of datapoints as the next highest class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1580fe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_small(stats, paths, size=256):\n",
    "    crit_met = np.logical_and(\n",
    "        stats[:, 0] >= size/2, stats[:, 1] >= size/2)\n",
    "    return stats[crit_met], paths[crit_met]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdc46f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats[(stats[:,2] < 11) & (stats[:, 2] > 6), 2] = 7\n",
    "stats[stats[:,2] >= 11, 2] = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bdb5fb",
   "metadata": {},
   "source": [
    "From here on, label 7 will represent 7-10 people, and label 8 will denote > 10 people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a9f0084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9475, 3)\n",
      "(9233, 3) (9233,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "filtered_stats, filtered_paths = filter_small(stats, paths, 224)\n",
    "sns.histplot(x=filtered_stats[:, 2])\n",
    "print(stats.shape)\n",
    "print(filtered_stats.shape, filtered_paths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f299ffff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def undersample(stats, paths):\n",
    "    labels = np.unique(stats[:, 2])\n",
    "    count = []\n",
    "    for label in labels:\n",
    "        count.append(np.count_nonzero(stats[:, 2] == label))\n",
    "    count.sort()\n",
    "    \n",
    "    retain = count[-4]\n",
    "    filtered = []\n",
    "    filtered_paths = []\n",
    "    for label in labels:\n",
    "        \n",
    "        # We scale the aspect ratios so that 1 -> 0, then \n",
    "        indices = (stats[:, 2] == label)\n",
    "        stats_temp = stats[indices]\n",
    "        paths_temp = paths[indices]\n",
    "        \n",
    "        ratio = np.abs(get_ratio(stats_temp) - 1)\n",
    "        kept_indices = np.argsort(ratio)[: retain]\n",
    "        filtered.append(stats_temp[kept_indices])\n",
    "        filtered_paths.append(paths_temp[kept_indices])\n",
    "    \n",
    "    filtered = np.concatenate(filtered, axis=0)\n",
    "    filtered_paths = np.concatenate(filtered_paths, axis=0)\n",
    "    return filtered, filtered_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "379f99c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, images_paths = undersample(filtered_stats, filtered_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29eadada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(x=images[:, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a42aaf",
   "metadata": {},
   "source": [
    "### Normalising Size of images\n",
    "\n",
    "1. We will be reading the images, converting them to the same size of 224x224, by scaling while retaining aspect ratio, then padding to a square with 0s.\n",
    "2. If the original image is larger than 256 on either side, it will be scaled down, else the size will remain the same.\n",
    "3. The dataset will then be saved to a .pt file, and uploaded to a repository so that it can be reproduced easier, instead of uploading all the images as a dataset.\n",
    "4. This step will only be performed on a local computer with the dataset and does not need to be reproduced when reproducing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1c4ee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_square(im, size=256):\n",
    "    x, y = im.size\n",
    "    factor = x / y\n",
    "    new_size = (size, size)\n",
    "    if x < y:\n",
    "        new_size = (int(factor*size), size)\n",
    "    elif x > y:\n",
    "        new_size = (size, int(size / factor))\n",
    "    new_im = Image.new('RGB', (size, size), color=0)\n",
    "    new_im.paste(im.resize(new_size),\n",
    "                 (int((size - new_size[0]) / 2), int((size - new_size[1]) / 2)))\n",
    "    return new_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a64eb5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(entries, paths, size=256):\n",
    "    transformer = transforms.ToTensor()\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    for entry, file_path in zip(entries, paths):\n",
    "        label = entry[2]\n",
    "        \n",
    "        im = Image.open(file_path)\n",
    "        new_im = make_square(im, size)\n",
    "        \n",
    "        dataset.append(transformer(new_im))\n",
    "        labels.append(label)\n",
    "        \n",
    "    dataset = torch.stack(dataset, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a106275c",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 224\n",
    "dataset, labels = load_dataset(images, images_paths, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8aaaa7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6333, 3, 224, 224])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4754c38a",
   "metadata": {},
   "source": [
    "### Saving as a .pt file\n",
    "\n",
    "The upload limit to github is 100mb per file, so we will save multiple instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4f93fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = dataset.size(0)\n",
    "train_indices = train_indices[train_indices < total_size]\n",
    "test_indices = test_indices[test_indices < total_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "932b52ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(dataset, name, indices):\n",
    "    batch_size = 170\n",
    "    total = indices.size\n",
    "    for count in range(0, total, batch_size):\n",
    "        batch_indices = indices[count: count+batch_size]\n",
    "        batch = None\n",
    "        if 'labels' in name:\n",
    "            batch = dataset[batch_indices]\n",
    "        elif 'data' in name:\n",
    "            batch = dataset[batch_indices, :, :, :]\n",
    "        \n",
    "        if batch != None:\n",
    "            torch.save(batch, \n",
    "                       f'./dataset/{name}/{name}_{(count//batch_size) + 1}.pt')\n",
    "        else:\n",
    "            print('Wrong name given')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fd56f269",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(dataset, 'train_data', train_indices)\n",
    "save_data(labels, 'train_labels', train_indices)\n",
    "save_data(dataset, 'test_data', test_indices)\n",
    "save_data(labels, 'test_labels', test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6710806",
   "metadata": {},
   "source": [
    "## Training of the model (Basic CNN)\n",
    "\n",
    "The main goals of this step is to\n",
    "1. Perform a Vanilla CNN\n",
    "2. Try to develop a more advanced structure mainly based on the hierarichal features idea taught in class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55786ed",
   "metadata": {},
   "source": [
    "### Getting dataset\n",
    "\n",
    "Using the requests module to download the dataset from our repo. (For use when running the notebook from collab where the files have yet to be downloaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ac791ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4098ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {\n",
    "    'train_data': [\n",
    "        'https://github.com/ClementNgZiQian/CS4243_2122S2_Project/blob/main/dataset/train_data/train_data_1.pt?raw=true',\n",
    "        30\n",
    "    ],\n",
    "    'train_labels': [\n",
    "        'https://github.com/ClementNgZiQian/CS4243_2122S2_Project/blob/main/dataset/train_labels/train_labels_1.pt?raw=true',\n",
    "        30\n",
    "    ],\n",
    "    'test_data': [\n",
    "        'https://github.com/ClementNgZiQian/CS4243_2122S2_Project/blob/main/dataset/test_data/test_data_1.pt?raw=true',\n",
    "        8\n",
    "    ],\n",
    "    'test_labels': [\n",
    "        'https://github.com/ClementNgZiQian/CS4243_2122S2_Project/blob/main/dataset/test_labels/test_labels_1.pt?raw=true',\n",
    "        8\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f20699e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "base_folder = './dataset'\n",
    "\n",
    "def check_dataset(files):\n",
    "    for type_, info in files.items():\n",
    "        if not os.path.exists(os.path.join(base_folder, type_)):\n",
    "            os.makedirs(os.path.join(base_folder, type_))\n",
    "        url = info[0]\n",
    "        \n",
    "        for i in range(info[-1]):\n",
    "            name = f'{type_}_{i+1}'\n",
    "            file_path = f'{base_folder}/{type_}/{name}.pt'\n",
    "            if not os.path.exists(file_path):\n",
    "                r = requests.get(url.replace(f'{type_}_1', name))\n",
    "                with open(file_path, 'wb') as f:\n",
    "                    f.write(r.content)\n",
    "\n",
    "not_found = check_dataset(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9357ea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(type_):\n",
    "    folder = os.path.join(base_folder, type_)\n",
    "    files = os.listdir(folder)\n",
    "    files = [os.path.join(folder, i) for i in files]\n",
    "    files.sort()\n",
    "    result = torch.load(files[0])\n",
    "    for file in files[1:]:\n",
    "        result = torch.cat((result, torch.load(file)), dim=0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea3221d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_dataset('train_data')\n",
    "train_label = load_dataset('train_labels')\n",
    "test_data = load_dataset('test_data')\n",
    "test_label = load_dataset('test_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d7cc981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5066, 3, 224, 224])\n",
      "torch.Size([1267, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ab1d82",
   "metadata": {},
   "source": [
    "### Helper functions for training model multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa32a204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(scores, labels):\n",
    "    bs = labels.size(0)\n",
    "    predicted_labels = scores.argmax(dim=1)\n",
    "    indicator = (predicted_labels == labels)\n",
    "    num_error = indicator.sum()\n",
    "    return num_error.float()/bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "295e4e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, train_data, train_label, \n",
    "                epochs, normalize=True, bs=100, lr=0.1):\n",
    "    \n",
    "    #torch.autograd.set_detect_anomaly(True)\n",
    "    start = time.time()\n",
    "    train_size = train_data.size(0)\n",
    "    num_batches = (train_size // bs) + 1\n",
    "    losses = np.zeros([epochs])\n",
    "    accuracies = np.zeros([epochs])\n",
    "    \n",
    "    mean = train_data.mean()\n",
    "    mean = mean.to(device)\n",
    "    std = train_data.std()\n",
    "    std = std.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_accuracy = 0\n",
    "        \n",
    "        if epoch % 10 == 9:\n",
    "            lr = lr / 2\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        shuffled_indices = torch.randperm(train_size)\n",
    "        \n",
    "        for count in range(0, train_size, bs):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            indices = shuffled_indices[count: count+bs]\n",
    "            minibatch_data = train_data[indices]\n",
    "            minibatch_label = train_label[indices]\n",
    "            \n",
    "            minibatch_data = minibatch_data.to(device)\n",
    "            minibatch_label = minibatch_label.to(device)\n",
    "            \n",
    "            if normalize:\n",
    "                minibatch_data = (minibatch_data - mean) / std\n",
    "            \n",
    "            minibatch_data.requires_grad_()\n",
    "            \n",
    "            scores = model(minibatch_data)\n",
    "            loss = criterion(scores, minibatch_label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.detach().item()\n",
    "            epoch_accuracy += get_accuracy(scores, minibatch_label).item()\n",
    "        \n",
    "        accuracies[epoch] = epoch_accuracy/num_batches\n",
    "        \n",
    "        losses[epoch] = epoch_loss/num_batches\n",
    "        \n",
    "        if epoch % 10 == 9:\n",
    "            elapsed = time.time() - start\n",
    "            print(f'Epoch = {epoch}, Time = {elapsed/60:.3f}min, \\\n",
    "              Loss={losses[epoch]:.3f}, Accuracy={accuracies[epoch]*100:.3f}%')\n",
    "    \n",
    "    return model, accuracies, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d906581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_data, test_label, \n",
    "               bs=100, normalize=True):\n",
    "    \n",
    "    accuracy = 0\n",
    "    test_size = test_label.size(0)\n",
    "    num_batches = (test_size // bs) + 1\n",
    "    \n",
    "    mean = train_data.mean()\n",
    "    mean = mean.to(device)\n",
    "    std = train_data.std()\n",
    "    std = std.to(device)\n",
    "    \n",
    "    for count in range(0, test_size, bs):\n",
    "        minibatch_data = test_data[count: count+bs]\n",
    "        minibatch_label = test_label[count: count+bs]\n",
    "        \n",
    "        minibatch_data = minibatch_data.to(device)\n",
    "        minibatch_label = minibatch_label.to(device)\n",
    "        \n",
    "        if normalize:\n",
    "            minibatch_data = (minibatch_data - mean) / std\n",
    "        \n",
    "        scores = model(minibatch_data)\n",
    "        accuracy += get_accuracy(scores, minibatch_label).item()\n",
    "    \n",
    "    accuracy = accuracy / num_batches\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc47ffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracies(train_accuracy, test_accuracy):\n",
    "    plt.plot(train_accuracy, label='Train Accuracy')\n",
    "    plt.plot(test_accuracy * np.ones(train_accuracy.shape), label='Test Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daa83c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ae9fb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(model, test_data, test_label, device):\n",
    "    \n",
    "    test_size = test_label.size(0)\n",
    "    \n",
    "    mean = test_data.mean()\n",
    "    mean = mean.to(device)\n",
    "    std = test_data.std()\n",
    "    std = std.to(device)\n",
    "    accuracy = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    \n",
    "    inputs = test_data.to(device)\n",
    "    test_label = test_label.to(device)\n",
    "    inputs = (inputs - mean) / std\n",
    "    scores = model(inputs)  \n",
    "    predicted_label = scores.cpu().argmax(dim=1).detach().numpy()\n",
    "    test_labels = test_label.cpu().detach().numpy()\n",
    "    \n",
    "    accuracy += accuracy_score(predicted_label, test_label)\n",
    "    precision += precision_score(predicted_label, test_label, average = 'weighted')\n",
    "    recall += recall_score(predicted_label, test_label, average = 'weighted')\n",
    "    f1 += f1_score(predicted_label, test_label, average = 'weighted')\n",
    "    \n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cccfb1e",
   "metadata": {},
   "source": [
    "We had planned to use k-fold to do hyper parmas searching, but it ended up requiring too much memory and there was insufficient time for us to find a solution, so we decided to not include it in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8646ed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "        \n",
    "def k_fold(model, k, train_data, train_label, criterion, device, epochs):\n",
    "    cv = 0\n",
    "    size = train_label.size\n",
    "    model.apply(reset_weights)\n",
    "    shuffled_indices = torch.randperm(size)\n",
    "    \n",
    "    for i in range(k):\n",
    "        test_indices = shuffled_indices[i*(size//k): (i+1)*(size//k)]\n",
    "        train_indices = torch.cat((shuffled_indices[: i*(size//k)], \\\n",
    "                                   shuffled_indices[(i+1)*(size//k):]), axis=0)\n",
    "        \n",
    "        train_data_k = train_data[train_indices]\n",
    "        train_label_k = train_label[train_indices]\n",
    "        test_data_k = train_data[test_indices]\n",
    "        test_label_k = train_label[test_indices]\n",
    "        \n",
    "        model, train_accuracy, train_loss = \\\n",
    "            train_model(model, criterion, train_data_k, train_label_k)\n",
    "        \n",
    "        cv += test_model(model, test_data_k, test_label_k)\n",
    "    \n",
    "    return cv/k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7123523",
   "metadata": {},
   "source": [
    "### Vanilla CNN\n",
    "\n",
    "We will begin with a simple CNN model, with 1 convolutional and linear layer each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "125dd2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import time\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b89f464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = np.array([224, 224])\n",
    "\n",
    "class Vanilla_CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        super(Vanilla_CNN, self).__init__()\n",
    "        \n",
    "        self.linear_dim = hidden_dim*size.prod()\n",
    "        self.conv = nn.Conv2d(3, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.linear = nn.Linear(self.linear_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 3 x 224 x 224 -> hidden_dim x 224 x 224\n",
    "        x = self.conv(x)\n",
    "        x = torch.relu(x)\n",
    "        # hidden_dim x 224 x 224 -> hidden_dim*224*224\n",
    "        x = x.view(-1, self.linear_dim)\n",
    "        # hidden_dim*224*224 -> 9\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efa26e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "van_net = Vanilla_CNN(32, 9)\n",
    "van_net = van_net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4bd3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "van_net, train_accuracy, train_loss = \\\n",
    "    train_model(van_net, criterion, train_data, train_label, epochs=100, lr=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30425c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = test_model(van_net, test_data, test_label)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37f1785",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracies(train_accuracy, test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3bf47a",
   "metadata": {},
   "source": [
    "### Simplification of the problem\n",
    "\n",
    "After consulting out TA on why our test error is still high even though our training loss was low, we were advised to simplify our problem into a 5-class classification where we classified images with at most 4 people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b95a809",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train = train_label <= 4\n",
    "mask_test = test_label <= 4\n",
    "\n",
    "\n",
    "train_label_2 = train_label[mask_train]\n",
    "train_data_2 = train_data[mask_train]\n",
    "test_label_2 = test_label[mask_test]\n",
    "test_data_2 = test_data[mask_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9b0e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_2.shape)\n",
    "print(test_data_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854ef14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "van_net = Vanilla_CNN(32, 5)\n",
    "van_net = van_net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a61e3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "van_net, train_accuracy, train_loss = \\\n",
    "    train_model(van_net, criterion, train_data_2, train_label_2, epochs=100, lr=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f078813",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = test_model(van_net, test_data_2, test_label_2)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00423e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracies(train_accuracy, test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d238c5",
   "metadata": {},
   "source": [
    "Noting that there was not a significant increase in performance, we tried to add more layers to the Vanilla CNN (4 convolutional layers) and we set the hidden_dim to be a hyperparameter. The idea behind using 4 layers is because of hierarichal features, as layer 3 is the minimum to guarantee complex features, we decided to go with 4 because clearly people counting is as complex if not more complex than normal image identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99162cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class new_Vanilla_CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        super(new_Vanilla_CNN, self).__init__()\n",
    "        \n",
    "        self.linear_dim = hidden_dim*8*14*14\n",
    "        self.conv1 = nn.Conv2d(3, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(hidden_dim, hidden_dim * 2, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(hidden_dim * 2, hidden_dim * 4, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(hidden_dim * 4, hidden_dim * 8, kernel_size=3, padding=1)\n",
    "        self.linear = nn.Linear(self.linear_dim, output_dim)\n",
    "        self.pool1 = nn.MaxPool2d(2,2)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        self.pool3 = nn.MaxPool2d(2,2)\n",
    "        self.pool4 = nn.MaxPool2d(2,2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 3 x 224 x 224 -> hidden_dim x 112 x 112\n",
    "        x = self.conv1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        # hidden_dim x 112 x 112 -> hidden_dim * 2 x 56 x 56\n",
    "        x = self.conv2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        # hidden_dim * 2 x 56 x 56 -> hidden_dim * 4 x 28 x 28\n",
    "        x = self.conv3(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        # hidden_dim * 4 x 28 x 28 -> hidden_dim * 8 x 14 x 14\n",
    "        x = self.conv4(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool4(x)\n",
    "        # hidden_dim * 8 x 14 x 14 -> hidden_dim*8*14*14\n",
    "        x = x.view(-1, self.linear_dim)\n",
    "        # hidden_dim*8*14*14 -> 9\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fd9263",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_van_net = new_Vanilla_CNN(32, 5)\n",
    "new_van_net = new_van_net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a8779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_van_net, train_accuracy, train_loss = \\\n",
    "    train_model(new_van_net, criterion, train_data_2, train_label_2, epochs=100, lr=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9db1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = test_model(new_van_net, test_data_2, test_label_2)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a48919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracies(train_accuracy, test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809f0d7f",
   "metadata": {},
   "source": [
    "### Multi-column CNN (MCNN)\n",
    "\n",
    "Observing that there was not much improvement in the performance of the model, we try a Multi-column CNN which should perform better than a vanilla CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7a098881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden1, hidden2, hidden3, output_dim):\n",
    "        super(MCNN, self).__init__()\n",
    "        \n",
    "        self.linear_dim = 56**2\n",
    "        \n",
    "        self.linear = nn.Linear(self.linear_dim, output_dim)\n",
    "        \n",
    "        self.branch1 = nn.Sequential(nn.Conv2d(3, hidden1, kernel_size= 9, padding=4),\n",
    "                                     nn.MaxPool2d(2,2),\n",
    "                                     nn.Conv2d(hidden1, hidden1*2, kernel_size= 7, padding=3),\n",
    "                                     nn.MaxPool2d(2,2),\n",
    "                                     nn.Conv2d(hidden1*2, hidden1, kernel_size= 7, padding=3),\n",
    "                                     nn.Conv2d(hidden1,  hidden1//2, kernel_size= 7, padding=3))\n",
    "        \n",
    "        self.branch2 = nn.Sequential(nn.Conv2d(3, hidden2, kernel_size= 7, padding=3),\n",
    "                                     nn.MaxPool2d(2,2),\n",
    "                                     nn.Conv2d(hidden2, hidden2*2, kernel_size= 5 ,padding=2),\n",
    "                                     nn.MaxPool2d(2,2),\n",
    "                                     nn.Conv2d(hidden2*2, hidden2, kernel_size= 5 ,padding=2),\n",
    "                                     nn.Conv2d(hidden2, hidden2//2, kernel_size= 5 ,padding=2))\n",
    "        \n",
    "        self.branch3 = nn.Sequential(nn.Conv2d(3, hidden3, kernel_size= 5 ,padding=2),\n",
    "                                     nn.MaxPool2d(2,2),\n",
    "                                     nn.Conv2d(hidden3, hidden3*2, kernel_size= 3 ,padding=1),\n",
    "                                     nn.MaxPool2d(2,2),\n",
    "                                     nn.Conv2d(hidden3*2, hidden3, kernel_size= 3 ,padding=1),\n",
    "                                     nn.Conv2d(hidden3, hidden3//2, kernel_size= 3 ,padding=1))\n",
    "        \n",
    "        self.fuse = nn.Sequential(nn.Conv2d((hidden1+hidden2+hidden3)//2, 1, kernel_size= 1, padding=0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "        x = torch.cat((x1,x2,x3), 1)\n",
    "        x = self.fuse(x)\n",
    "        x = torch.relu(x)\n",
    "        x = x.view(-1, self.linear_dim)\n",
    "        x = self.linear(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a6804bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_net = MCNN(16, 20, 24, 5)\n",
    "m_net = m_net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe2a940",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_net, train_accuracy, train_loss = \\\n",
    "    train_model(m_net, criterion, train_data_2, train_label_2, epochs=20, lr=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e40dee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = test_model(m_net, test_data_2, test_label_2)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb85d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracies(train_accuracy, test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f847552c",
   "metadata": {},
   "source": [
    "## Object Detection / Image Segmentation\n",
    "\n",
    "The main goals of this step is to\n",
    "1. Perform a Faster-RCNN\n",
    "2. Test for the accuracy of this model compared to the network we created, for the purpose of the research of future improvements that can be integrated into our network to improve the accuracy rates\n",
    "\n",
    "At this point, we realised that the most probable reason that the network still failed was due to it being a normal CNN and not object detection model.\n",
    "1. Counting people would first require the network to identify which sections have people, and then from there count how many people. \n",
    "2. Our data is not labelled with bounding boxes due to time constraint and difficulty in labelling so many images with just 3 people, we have decided to try use a pretrained object detector model and from there use its results to count the number of people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd2a756",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyyaml==5.1\n",
    "\n",
    "import torch\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "# Install detectron2 that matches the above pytorch version\n",
    "# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
    "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/$CUDA_VERSION/torch$TORCH_VERSION/index.html\n",
    "# If there is not yet a detectron2 release that matches the given torch + CUDA version, you need to install a different pytorch.\n",
    "\n",
    "#exit(0)  # After installation, you may need to \"restart runtime\" in Colab. This line can also restart runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c9c338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "#from google.colab.patches import cv2_imshow\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bdb665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('./dataset_png'):\n",
    "    !wget https://github.com/ClementNgZiQian/CS4243_2122S2_Project/releases/download/v1.0/dataset_png.zip \n",
    "    !unzip dataset_png.zip > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4801d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectron_count(predictor, base_folder, class_number):\n",
    "    \n",
    "    batch_folders = os.listdir(base_folder)\n",
    "    correct = 0\n",
    "    total_size = 0\n",
    "    for folder in batch_folders:\n",
    "        folder_path = f'{base_folder}/{folder}'\n",
    "        images = os.listdir(folder_path)\n",
    "        labels_path = folder_path.replace('data', 'labels') + '.pt'\n",
    "        labels = torch.load(labels_path)\n",
    "        total_size += len(labels)\n",
    "        for im_path, label in zip(images, labels):\n",
    "            im = cv2.imread(f'{folder_path}/{im_path}')\n",
    "        \n",
    "            outputs = predictor(im)\n",
    "            preds = outputs['instances'].pred_classes.to('cpu')\n",
    "            count = torch.count_nonzero(preds == class_number)  \n",
    "            correct += (count == label)\n",
    "\n",
    "    accuracy = (correct/total_size) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56fd692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inf(pre_trained, ROI):\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(pre_trained))\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = ROI  # set threshold for this model\n",
    "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(pre_trained)\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "    train_accuracy = detectron_count(predictor, './dataset_png/train_data', 0)\n",
    "    test_accuracy = detectron_count(predictor, './dataset_png/test_data', 0)\n",
    "    return train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236bc205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracies_detectron(test_accuracy, train_accuracy):\n",
    "    plt.plot(train_accuracy * np.ones(10), label='Train Accuracy')\n",
    "    plt.plot(test_accuracy * np.ones(10), label='Test Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97948c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rmdir './dataset_png/train_labels/.ipynb_checkpoints'\n",
    "!rmdir './dataset_png/test_labels/.ipynb_checkpoints'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93da53bb",
   "metadata": {},
   "source": [
    "#### Running Inference using - **COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml**\n",
    "\n",
    "We create a detectron2 config and a detectron2 `DefaultPredictor` to run inference on image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f5fa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROI = 0.5\n",
    "start = time.time()\n",
    "accuracy_detectron_2train, accuracy_detectron_2test = \\\n",
    "        run_inf(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\", ROI)\n",
    "end = time.time()\n",
    "print(f'Train set accuracy = {accuracy_detectron_2train:.3f}%, \\\n",
    "    Test set accuracy = {accuracy_detectron_2test:.3f}%, \\\n",
    "    time taken = {(end - start)/60:.3f}mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6fcc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracies_detectron(accuracy_detectron_2test/100, accuracy_detectron_2train/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15f708f",
   "metadata": {},
   "source": [
    "#### Running Inference using - **COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml**\n",
    "\n",
    "We create a detectron2 config and a detectron2 `DefaultPredictor` to run inference on image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3831ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROI = 0.5\n",
    "start = time.time()\n",
    "accuracy_detectron_2train, accuracy_detectron_2test = \\\n",
    "        run_inf(\"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\", ROI)\n",
    "end = time.time()\n",
    "print(f'Train set accuracy = {accuracy_detectron_2train:.3f}%, \\\n",
    "    Test set accuracy = {accuracy_detectron_2test:.3f}%, \\\n",
    "    time taken = {(end - start)/60:.3f}mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c381cfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracies_detectron(accuracy_detectron_2test/100, accuracy_detectron_2train/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc598ce5",
   "metadata": {},
   "source": [
    "## Our conclusion:\n",
    "\n",
    "From the results above, it shows the image segmentation model giving much higher accuracy rates, which were around 59-64%, as compared to the other networks which were trained on our dataset, which gave lower accuracy rates of around 20%. From here, our team has concluded that image segmentation model could be integrated into our future implementations as part of the solution to our intial problem.\n",
    "\n",
    "Additionally, the results displayed above has shown a higher accuracy rate for the 2nd model, which  as compared to the 1st model. From here, our team learned that the difference in learning rate schedules affects the accuracy rates predicted by the model. The results has shown that the models trained with a higher number of epochs/iterations gave predictions with higher accuracy rates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
